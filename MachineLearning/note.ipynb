{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RnR-hyyXN4i"
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "kz9uhFIqXN4l"
   },
   "source": [
    "## 1.2 Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkK1295YXN4n"
   },
   "source": [
    "### 1.2.2 Expectations and covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbgWvQLsXN4p"
   },
   "source": [
    "The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the _expectation_ of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-d08riKXN4q"
   },
   "source": [
    "For a discrete distribution\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[f] = \\sum_{x}p(x)f(x) \\tag{1.33}\n",
    "$$\n",
    "For a continuous distribution\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[f] = \\int p(x)f(x)dx \\tag{1.34}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vewHoWMXN4s"
   },
   "source": [
    "If we are given a finite number N of points drawn from the probability distribution or probability density, the the _expectation_ can be approximated as a finite sum over these points. The approximation becomes exact in the limit $N\\to\\infty$ \n",
    "$$\\mathop{\\mathbb{E}}[f] \\simeq \\frac{1}{N}\\sum^{N}_{n=1}f(x_{n}) \\tag{1.35}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChQnb42RXN4t"
   },
   "source": [
    "We use a subcript to indicate which variable is being averaged over a functions of several variables. So the _expectation_ of the function $f(x, y)$ with respect to the distribution of $x$ is denoted by $$\\mathop{\\mathbb{E}_{x}}[f(x, y)] \\tag{1.36}$$ \n",
    "Note $\\mathop{\\mathbb{E}_{x}}[f(x, y)]$ will be a function of $y$.\n",
    "And we use $\\mathop{\\mathbb{E}_{x}}[f\\mid y]$ to denote a _conditional expectation_ with repect to a conditional distribution.\n",
    "$$\\mathop{\\mathbb{E}_{x}}[f\\mid y] = \\sum_{x}p(x\\mid y)f(x) = \\int p(x\\mid y)f(x)dx \\tag{1.37}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVrzT4ObXN4v"
   },
   "source": [
    "The covariance and variance is defined by\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "cov(f,g) & = \\mathop{\\mathbb{E}_{x, y}}\\big[(f(x) - \\mathop{\\mathbb{E}}[f(x)])(g(y) - \\mathop{\\mathbb{E}}[g(y)])\\big]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}\\big[(f(x)g(y)\\big] - \\mathop{\\mathbb{E}_{x, y}}\\big[f(x)\\mathop{\\mathbb{E}}[g(y)]\\big] - \\mathop{\\mathbb{E}_{x, y}}\\big[g(y)\\mathop{\\mathbb{E}}[f(x)]\\big] + \\mathop{\\mathbb{E}_{x, y}}\\big[\\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\big]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}[(f(x)g(y)] - \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)] - \\mathop{\\mathbb{E}}[g(y)]\\mathop{\\mathbb{E}}[f(x)] + \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}[(f(x)g(y)] - \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQOI8JQEXN4w"
   },
   "source": [
    "$$var(f) = \\mathop{\\mathbb{E}}\\big[\\big(f(x) - \\mathop{\\mathbb{E}}[f(x)]\\big)^{2}\\big] \\tag{1.38}$$\n",
    "$$var(f) = \\mathop{\\mathbb{E}}[(f(x)^{2}] - \\mathop{\\mathbb{E}}[f(x)]^{2} \\tag{1.39}$$\n",
    "If $f(x) = x$, $g(y) = y$\n",
    "$$var(x) = cov(x, x) = \\mathop{\\mathbb{E}}[x^{2}] - \\mathop{\\mathbb{E}}[x]^{2} \\tag{1.40}$$\n",
    "$$cov(x, y) = \\mathop{\\mathbb{E}_{x, y}}[xy] - \\mathop{\\mathbb{E}}[x]\\mathop{\\mathbb{E}}[y] \\tag{1.41}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q70abmPsXN4y"
   },
   "source": [
    "for $\\textbf{x}\\in R^{m}$ and $\\textbf{y}\\in R^{n}$, the result is a matrix.\n",
    "$$\n",
    "cov(\\textbf{x}, \\textbf{y}) = \\mathop{\\mathbb{E}_{\\textbf{x}, \\textbf{y}}}[\\textbf{x}\\textbf{y}^{T}] - \\mathop{\\mathbb{E}}[\\textbf{x}]\\mathop{\\mathbb{E}}[\\textbf{y}]^{T} \\tag{1.42}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUe9Q1TKXN40"
   },
   "source": [
    "The covariance matrix generalizes the notion of variance to multiple dimensions.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Sigma(\\textbf{x}) & = cov(\\textbf{x}, \\textbf{x})\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg]\\\\\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg] \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg]\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Droabe_wXN41"
   },
   "source": [
    "### 1.2.3 Bayesian probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPkpATX0XN43"
   },
   "source": [
    "Bayes's theorem allows us to evaluate the uncertainty in $\\textbf{w}$ in the form of the posterior probability $p(\\textbf{w}\\mid \\mathcal{D})$ after we have incorporated the evidence provided by the observed data $\\mathcal{D}$.\n",
    "$$p(\\textbf{w}\\mid \\mathcal{D}) = \\frac{p(\\mathcal{D}\\mid\\textbf{w})p(\\textbf{w})}{p(\\mathcal{D})} \\tag{1.43}$$\n",
    "The quantity $p(\\mathcal{D}\\mid\\textbf{w})$ is called the _likelihood function_. It expresses how probable the observed data ser is for the specified parameter vector $\\textbf{w}$. $p(\\textbf{w})$ is the prior probability distribution of $\\textbf{w}$ before observing the data.\n",
    "We can state Bayes's theorem in words\n",
    "$$posterior \\propto likelihood \\times prior \\tag{1.44}$$\n",
    "The denominator is the normalization constant. which ensures that the posterior distribution integrates to one.\n",
    "$$p(\\mathcal{D}) = \\int p(\\mathcal{D}\\mid\\textbf{w})p(\\textbf{w})d\\textbf{w} \\tag{1.45}$$\n",
    "In a frequentist setting, $\\textbf{w}$ is determined by some form of \"estimator\". A widely used one is _maximum likelihood_, in which $\\textbf{w}$ is set to the value that maximizes the likelihood function $p(\\mathcal{D}\\mid\\textbf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTMKbjhzXN44"
   },
   "source": [
    "### 1.2.4 The Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kIcMgynXN46"
   },
   "source": [
    "For the case of a single real-value variable $x$, the Gaussian distribution is defined by\n",
    "$$\\mathcal{N}(x\\mid\\mu, \\sigma^{2}) = \\frac{1}{(2\\pi\\sigma^{2})^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma^{2}}(x - \\mu)^{2}\\right\\} \\tag{1.46}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIA29iupXN48"
   },
   "source": [
    "$$\\mathop{\\mathbb{E}}[x] = \\mu = \\text{\"mean\"}\\tag{1.49}$$\n",
    "$$\\mathop{\\mathbb{E}}[x^{2}] = \\mu^{2} + \\sigma^{2} \\tag{1.50}$$\n",
    "$$var[x] = \\mathop{\\mathbb{E}}[x^{2}] - \\mathop{\\mathbb{E}}[x]^{2} = \\sigma^{2} =\\text{\"variance\"} \\tag{1.51}$$\n",
    "The reciprocal of the variance, written as $\\beta = \\frac{1}{\\sigma^{2}}$, is called the _precision_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4hclTZDXN49"
   },
   "source": [
    "$\\mathcal{N}$ defined over a D-dimensional vector x of continuous variables with the covariance $\\Sigma$ is given by\n",
    "$$\\mathcal{N}(x\\mid\\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{\\left|\\Sigma\\right|^{1/2}}exp\\left\\{-\\frac{1}{2}(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu)\\right\\} \\tag{1.52}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "KEPTjm0hXN4-"
   },
   "source": [
    "Suppose that we have a data set of N observation that are _independent and identically distributed_ $\\textbf{X}$, the using the fact that the _joint probability_ of two independet events is given by the product of marginal probabilities. The probability of the data set, given $\\mu$ and $\\sigma^{2}$ is\n",
    "$$p(\\textbf{X}\\mid\\mu, \\sigma^{2}) = \\prod^{N}_{n=1}\\mathcal{N}(x_{n}|\\mu, \\sigma^{2}) \\tag{1.53}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-Klb1KSXN4_"
   },
   "source": [
    "Taking the log of the likelihhod function, results in the form\n",
    "$$\n",
    "ln\\,p(\\textbf{X}\\mid\\mu, \\sigma^{2}) = -\\frac{1}{2\\sigma^{2}}\\sum^{N}_{n=1}(x_{n} - \\mu)^{2} - \\frac{N}{2} ln\\,\\sigma^{2} - \\frac{N}{2} ln\\,(2\\pi) \\tag{1.54}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB2_0xB4XN5A"
   },
   "source": [
    "Maximizing with respect to $\\mu$, we obtain the maximum likelihood solution which is the _sample mean_.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\mu} & = \\arg\\max_{\\mu}eq(1.54)\\\\\n",
    "\\implies\\frac{\\partial}{\\partial\\mu}eq(1.54) = 0 & \\implies\\sum^{N}_{n=1}x_{n} = N\\mu\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\\hat{\\mu} = \\frac{1}{N}\\sum^{N}_{n=1}x_{n} \\tag{1.55}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g04CVTaEXN5B"
   },
   "source": [
    "Similarly, maximizing eq(1.54) with respect to $\\sigma^{2}$, we can the _sample variance_.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\sigma}^{2} & = \\arg\\max_{\\sigma^{2}}eq(1.54)\\\\\n",
    "\\implies\\frac{\\partial}{\\partial\\sigma^{2}}eq(1.54) = 0 & \\implies\\frac{1}{2(\\sigma^{2})^{2}}\\sum^{N}_{n=1}(x_{n} - \\mu)^{2} = \\frac{N}{2\\sigma^{2}}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\\hat{\\sigma}^{2} = \\frac{1}{N}\\sum^{N}_{n=1}(x_{n} - \\hat{\\mu})^{2} \\tag{1.56}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRQIV_YlXN5C"
   },
   "source": [
    "$$\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\mu}] = \\mathop{\\mathbb{E}}\\bigg[\\frac{1}{N}\\sum^{N}_{n=1}x_{n}\\bigg] = \\frac{1}{N}\\sum^{N}_{n=1}\\mathop{\\mathbb{E}}[x_{n}] = \\mu \\tag{1.57}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VK_E010XN5D"
   },
   "source": [
    "The maximum likelihood approach systematically underestimates the variance of the distribution by a factor $(N - 1) / N$.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\sigma}^{2}] & = \\mathop{\\mathbb{E}}\\bigg[\\frac{1}{N}\\sum^{N}_{i=1}(x_{i} - \\frac{1}{N}\\sum^{N}_{j=1}x_{j})^{2}\\bigg]\\\\ \n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\mathop{\\mathbb{E}}\\bigg[x_{i}^{2} - \\frac{2}{N}x_{i}\\sum^{N}_{j=1}x_{j} + \\frac{1}{N^{2}}\\sum^{N}_{j=1}x_{j}\\sum^{N}_{k=1}x_{k}\\bigg]\\\\\n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\bigg[\\frac{N-2}{N}\\mathop{\\mathbb{E}}[x_{i}^{2}] - \\frac{2}{N}\\sum^{N}_{j\\neq i}\\mathop{\\mathbb{E}}[x_{i}x_{j}] + \\frac{1}{N^{2}}\\sum^{N}_{j=1}\\sum^{N}_{k\\neq j}\\mathop{\\mathbb{E}}[x_{j}x_{k}] + \\frac{1}{N^{2}}\\sum^{N}_{j=1}\\mathop{\\mathbb{E}}[x_{j}^{2}]\\bigg]\\\\\n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\bigg[\\frac{N-2}{N}(\\mu^{2} + \\sigma^{2}) - \\frac{2}{N}(N - 1)\\mu^{2} + \\frac{1}{N^{2}}N(N - 1)\\mu^{2} + \\frac{1}{N}(\\mu^{2} + \\sigma^{2})\\bigg]\\\\\n",
    "& = \\frac{N-1}{N}\\sigma^{2}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\sigma}^{2}] = \\frac{N-1}{N}\\sigma^{2} \\tag{1.58}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jM5Kx9p4XN5D"
   },
   "source": [
    "This is an example of a phenomenon called _bias_ and is related to the problem of over-fitting. The bias of $\\hat{\\theta}$ is defined as\n",
    "$$\n",
    "B(\\hat{\\theta}) = \\mathop{\\mathbb{E}}[\\hat{\\theta} - \\theta] = \\mathop{\\mathbb{E}}[\\hat{\\theta}] - \\theta\n",
    "$$\n",
    "The estimator $\\hat{\\theta}$ is an _unbiased estimator_ of $\\theta$ if and only if $B(\\hat{\\theta}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnmxRvipXN5E"
   },
   "source": [
    "From eq(1.58) it follows that the following estimate for the variance is unbiased\n",
    "$$\n",
    "s^{2} = \\tilde{\\sigma}^{2} = \\frac{N}{N-1}\\hat{\\sigma}^{2} = \\frac{1}{N-1}\\sum^{N}_{n=1}(x_{n} - \\hat{\\mu})^{2} \\tag{1.59}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWFZEU9bXN5F"
   },
   "source": [
    "The bias of the maximum lkelihood solution becomes less significant as the number N of data points icreases, and in the limit $N\\to\\infty$ the maximum likelihood solution for the variance euqals the true variance of the distribution that enerated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjIEhvfXN5G"
   },
   "source": [
    "### 1.2.5 Curve fitting re-visited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3Nyty_tXN5H"
   },
   "source": [
    "We assume that given the value of _x_, the corresponding value of _t_ has a Gaussian distribution with a mean equal to the value $y(x,\\textbf{w})$. Thus we can express our uncertainty over the value of the target variable using a probability distribution.\n",
    "$$\n",
    "p(t\\mid x, \\textbf{w}, \\beta) = \\mathcal{N}(t\\mid y(x, \\textbf{w}), \\beta^{-1}) \\tag{1.60}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUG9fTkXN5I"
   },
   "source": [
    "We use the training data $\\{\\textbf{x},\\textbf{w}\\}$ to determine the values of the unknown parameters $\\textbf{w}$ and $\\beta$ by maximum likelihood. If the data are assumed to be drawn independently from the distribution, then the likelihood function is given by\n",
    "$$\n",
    "p(\\textbf{t}\\mid\\textbf{x},\\textbf{w},\\beta) = \\prod^{N}_{n=1}\\mathcal{N}(t_{n}\\mid y(x_{n}, \\textbf{w}), \\beta^{-1}) \\tag{1.61}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVyZ3c6WXN5K"
   },
   "source": [
    "Substituting for the form of the Gaussian distribution, and take the logarithm\n",
    "$$\n",
    "ln\\,p(\\textbf{t}\\mid\\textbf{x},\\textbf{w},\\beta) =  -\\frac{\\beta}{2}\\sum^{N}_{n=1}\\mathcal\\{y(x_{n}, \\textbf{w}) - t_{n}\\}^{2} + \\frac{N}{2}ln\\,\\beta - \\frac{N}{2}ln\\,(2\\pi) \\tag{1.62}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "uKzvLHMNXN5L"
   },
   "source": [
    "Maximizes likelihood with respect to $\\textbf{w}$ we can obtain the _sum-of-squares-error-function_ defined by eq(1.2). Maximizing likehood with respect to $\\beta$ gives\n",
    "$$\n",
    "\\frac{1}{\\hat{\\beta}} = \\frac{1}{N}\\sum^{N}_{n=1}\\{y(x_{n}, \\hat{\\textbf{w}}) - t_{n}\\}^{2} \\tag{1.63}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7dYr-CTXN5M"
   },
   "source": [
    "Having determined the parameters $\\textbf{w}$ and $\\beta$, we can express our probabilistic model in terms of the _predictive distribution_ that gives the probability distribution over _t_.\n",
    "$$p(t\\mid x,\\hat{\\textbf{w}},\\hat{\\beta}) = \\mathcal{N}(t\\mid y(x, \\hat{\\textbf{w}}), \\hat{\\beta}^{-1}) \\tag{1.64}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXV1xVAxXN5N"
   },
   "source": [
    "We introduce a prior distribution over the polynomial coefficients $\\textbf{w}$. Here use a Gaussian distribution just for simplicity.\n",
    "$$\n",
    "p(\\textbf{w}\\mid\\alpha) = \\mathcal{N}(\\textbf{w}\\mid 0, \\alpha^{-1}\\textbf{I}) = \\Big(\\frac{\\alpha}{2\\pi}\\Big)^{(M + 1) / 2} exp\\big\\{-\\frac{\\alpha}{2}\\textbf{w}^{T}\\textbf{w}\\big\\} \\tag{1.65}\n",
    "$$\n",
    "Where $\\alpha$ is the precision of the distribution, and $M + 1$ is the total number of elements in the vector $\\textbf{w}$ for an $M^{th}$ oder polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHeHdaNfXN5O"
   },
   "source": [
    "Variables such as $\\alpha$, which control the distribution of model parameters, are called _hyperparameters_. Using Bayes's theorem\n",
    "$$p(\\textbf{w}\\mid\\textbf{x}, \\textbf{t}, \\alpha, \\beta) \\propto p(\\textbf{t}\\mid\\textbf{x}, \\textbf{w}, \\beta)p(\\textbf{w}\\mid\\alpha)\\tag{1.66}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_p_V400XN5P"
   },
   "source": [
    "We can now determine $\\textbf{w}$ by finding the most probable value of $\\textbf{w}$ by maximizing the posterior distribution.\n",
    "Taking the negative logarithm of eq(1.66), we find that the maximum of the posterior is given by the minimum of\n",
    "$$\\frac{\\beta}{2}\\sum^{N}_{n=1}\\{y(x_{n}, \\hat{\\textbf{w}}) - t_{n}\\}^{2} + \\frac{\\alpha}{2}\\textbf{w}^{T}\\textbf{w}\\tag{1.67}$$\n",
    "eq(1.4) is equivalent to minimize above equation with a regularization parameter given $\\lambda = \\frac{\\alpha}{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1JOv041XbFW"
   },
   "source": [
    "TODO> PRML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVUG0AuJXwsF"
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZj4imExX3S8"
   },
   "source": [
    "## Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rY5SbMSAYJfp"
   },
   "source": [
    "A machine learning algorithm is a computer program which is said to learn from experience **E** with respect to someclass of tasks **T** and performance measure **P**, if its performance at tasks in **T**, asmeasured by **P**, improves with experience **E**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psv0NesfX-4k"
   },
   "source": [
    "### The Task, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpFvuYteY4GD"
   },
   "source": [
    "Machine learning tasks are usually described in terms of how the machine learning system should process an example. An example is a collection of features that have been quantitatively measured from some object or event that we want the machine learning system to process. We typically represent an example as avector $x \\in R^n$ where each entry $x_i$ of the vector is another feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyjLH_-FZZgk"
   },
   "source": [
    "#### Classiﬁcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-GmcGhUZiAp"
   },
   "source": [
    "The learning algorithm is usually asked to produce a function $f:R^n \\to \\{1, . . . , k\\}$ to specify which of $k$ categories some input belongs to. There are other variants where $f$ outputs a probability distribution over classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0HTdtTFaZwV"
   },
   "source": [
    "#### Classiﬁcation with missing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVDIxvkSaeC2"
   },
   "source": [
    "To solve the classiﬁcation task when some of the inputs may be missing, the learning algorithm must learn a set of functions. Each function corresponds to classifying $x$ with a diﬀerent subset of its inputs missing. One way to eﬃciently deﬁne such a large set of functions is to learn a probability distribution over all the relevant variables, then solve the classiﬁcation task by marginalizing out the missing variables. The computer program needs to learn only a single function describing the joint probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vx40DysXlUqA"
   },
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtHuX8SElW_U"
   },
   "source": [
    "The learning algorithmis asked to output a function $f:R^n→ R$ to predict a numerical value given some input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1BdyUgllyld"
   },
   "source": [
    "#### Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtiwkukmmIsK"
   },
   "source": [
    "The machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe the information into discrete textual form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3Qcn9yrm2HM"
   },
   "source": [
    "#### Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbMoiG-Im7nP"
   },
   "source": [
    "In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsWdkqBXnK3d"
   },
   "source": [
    "#### Structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxQhGIYin3cG"
   },
   "source": [
    "Structured output tasks involve any task where the output is a vector with important relationships between the diﬀerent elements. This is a broad category and subsumes the transcription and translation tasks described above, as well as many other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BUDpMGoMTmT"
   },
   "source": [
    "#### Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o65YgVWAMeCw"
   },
   "source": [
    "The computer program sifts through a set of events or objects and ﬂags some of them as being unusualor atypical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYV6lVFXNzTK"
   },
   "source": [
    "#### Synthesis and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5HAqCahN0pL"
   },
   "source": [
    "The machine learning al-gorithm is asked to generate new examples that are similar to those in thetraining data. In some cases, we want the sampling or synthesis procedure to generate aspeciﬁc kind of output given the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ2ymf2dOsA0"
   },
   "source": [
    "#### Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLLQhO5jOtkv"
   },
   "source": [
    "In this type of task, the machine learning algorithm is given a new example $x ∈ R^n$, but with some entries $x_i$ of $x$ missing. The algorithm must provide a prediction of the values of the missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqMupBVsPSW5"
   },
   "source": [
    "#### Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITJiKtavPTw-"
   },
   "source": [
    "The machine learning algorithm is given as input a corrupted example $\\tilde{x} ∈ R^n$ obtained by an unknown corruption process from a clean example $x ∈ R^n$. The learner must predict the clean examplex from its corrupted version $\\tilde{x}$, or more generally predict the conditional probability distribution $p(x | \\tilde{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w2_6vafQRML"
   },
   "source": [
    "#### Density estimationorprobability mass function estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIj_Mg7mQTaF"
   },
   "source": [
    "The machine learning algorithm is asked to learn a function $p_{model}:R^n \\to R$, where $p_{model}(x)$ can be interpreted as a probability density function (if $x$ is continuous) or a probability mass function (if $x$ is discrete) on the space that the examples were drawn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ys6T3wE0-E8h"
   },
   "source": [
    "### Performance Measure, $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXx9R7ul-Mc_"
   },
   "source": [
    "To evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure $P$ is speciﬁc to the task T being carried out by the system. For tasks such as classiﬁcation we often measure the **accuracy** or the **error rate** of the model. We often refer to the error rate as the expected **0-1 loss**. The **0-1 loss** on a particular example is 0 if it is correctly classiﬁed and 1 if it is not. For tasks such as density estimation, the most common approach is to report the average log-probability the model assigns to some examples. We therefore evaluate these performance measures usingatest setof data that is separate from the data used for training the machinelearning system. In other cases, we know what quantity we would ideally like to measure, but measuring it is impractical. In these cases, one must design a good approximation to the desired criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S_1fwFaJJnq"
   },
   "source": [
    "### The Experience, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrR5FxC8JheP"
   },
   "source": [
    "Machine learning algorithms can be broadly categorized as unsupervised or supervised. Most of the learning algorithms can be understood as being allowed to experience an entiredataset. A dataset is a collection of many examples, Sometimes we call examples data points.\n",
    "\n",
    "**Unsupervised learning** algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset.\n",
    "\n",
    "**Supervised learning algorithms** experience a dataset containing features, but each example is also associated with a label or target.\n",
    "\n",
    "Roughly speaking, unsupervised learning involves observing several examples of a random vector $x$ and attempting to implicitly or explicitly learn the probability distribution $p(x)$, or some interesting properties of that distribution; while supervised learning involves observing several examples of a random vector $x$ and an associated value or vectory, then learning to predicty from $x$, usually by estimating $p(y | x)$.\n",
    "\n",
    "The chain rule of probability states that for a vector $x \\in R^n$, the joint distribution can be decomposed as\n",
    "\n",
    "$$\n",
    "p(x) =\\prod_{i=1}^n{p(x_i| x_1, . . . , x_{i−1})} \n",
    "$$\n",
    "\n",
    "This decomposition means that we can solve the ostensibly unsupervised problem of modeling $p(x)$ by splitting it into $n$ supervised learning problems. Alternatively, we can solve the supervised learning problem of learning $p(y | x)$ by using traditional unsupervised learning technologies to learn the joint distribution $p(x, y)$, then inferring\n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{p(x, y)}{\\sum_{y\\prime}{p(x,y\\prime)}}\n",
    "$$\n",
    "\n",
    "One common way of describing a dataset is with a design matrix. A design matrix is a matrix containing a diﬀerent example in each row. Each column of the matrix corresponds to a diﬀerent feature. We also provide a vector of labels $y$, with $y_i$ providing the label for example $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjmC9Q4CCEQ-"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Z0Sg_AtCQ65"
   },
   "source": [
    "The goal is to build a system that can take a vector $x \\in R^n$ as input and predict the value of a scalar $y \\in R$ as its output. Let $\\tilde{y}$ be the value that our model predicts $y$ should take on.\n",
    "\n",
    "$$\n",
    "\\tilde{y} = w^{T}x\n",
    "$$\n",
    "\n",
    "where $w \\in R^n$ is a vector of **parameters**. We can think of $w$ as a set of weights that determine how each feature aﬀects the prediction.\n",
    "\n",
    "One way of measuring the performance of the model is to compute the mean squared error of the model on the test set.\n",
    "\n",
    "$$\n",
    "MSE_{test}=\\frac{1}{m}\\sum_i{(\\tilde{y}^{(test)}− y^{(test)})_i^2}.\n",
    "$$\n",
    "\n",
    "To minimize MSEtrain, we can simply solve for where its gradient is 0\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla w\\frac{1}{m} ||\\tilde{y}^{(train)}− y^{(train)}||^2_2= 0 \\implies\\\\\n",
    "\\frac{1}{m} \\nabla w ||X^{(train)}w− y^{(train)}||^2_2= 0 \\implies\\\\\n",
    "\\nabla w (X^{(train)}w− y^{(train)})^T (X^{(train)}w− y^{(train)})= 0 \\implies\\\\\n",
    "\\nabla w (w^T X^{(train)T} X^{(train)} w− 2w^T X^{(train)T} y^{(train)} + y^{(train)T} y^{(train)})= 0 \\implies\\\\\n",
    "2 X^{(train)T} X^{(train)} w − 2 X^{(train)T} y^{(train)}= 0 \\implies\\\\\n",
    "w = (X^{(train)T} X^{(train)})^{-1} X^{(train)T} y^{(train)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The equation is known as the normal equations. \n",
    "\n",
    "The term linear regression is often used to refer to a slightly more sophisticated model with one additional paramete an intercept term b.\n",
    "\n",
    "$$\n",
    "\\tilde{y} = w^T x +b\n",
    "$$\n",
    "\n",
    "Instead of adding the bias parameterb, one can continue to use the model with only weights but augment $x$ with anextra entry that is always set to 1. The weight corresponding to the extra 1 entry plays the role of the bias parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKcrFLULOc0W"
   },
   "source": [
    "## Capacity, Overﬁtting and Underﬁtting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbY0vpvkivG-"
   },
   "source": [
    "The ability to perform well on previously unobserved inputs is called generalization. Typically, when training a machine learning model, we can compute the training error on the training set, and we reduce this training error. The generalization error also called the test error is deﬁned as the expected value of the error on a new input. We typically estimate the generalization error of a machine learning model by measuring its performance on a test set of examples that were collected separately from the training set.\n",
    "\n",
    "The ﬁeld of **statistical learning theory** provides If the **data-generating distribution**, denoted $p_{data}$ is **i.i.d**, then the expected training error of a randomly selected model is equal to the expected test error of that model. After the process of training, we choose the parameters to reduce training set error, so the expected test error is greater than or equal to the expected value of training error.\n",
    "\n",
    "1. Make the training error small.\n",
    "2. Make the gap between training and test error small\n",
    "\n",
    "Will make algorithm to do well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpA6eQ95mXlq"
   },
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjrtmnTbnEai"
   },
   "source": [
    "Underﬁtting occurs when the model is not able to obtain a suﬃciently low error value on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bREL56LxmaLi"
   },
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyEnC_NmnH3w"
   },
   "source": [
    "Overﬁtting occurs whenthe gap between the training error and test error is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVwiLuF3nTt7"
   },
   "source": [
    "### Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhn-_52PnWTY"
   },
   "source": [
    "We can control whether a model is more likely to overﬁt or underﬁt by altering its capacity. Informally, a model’s capacity is its ability to ﬁt a wide variety of functions. Models with low capacity may struggle to ﬁt the training set. Modelswith high capacity can overﬁt by memorizing properties of the training set that donot serve them well on the test set. One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, we can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model’s capacity.\n",
    "\n",
    "We can add more powers of x as additional features, for example, to obtain a polynomial of degree 9:\n",
    "\n",
    "$$\n",
    "\\tilde{y} = \\sum_{i=1}^{9}{w_i x^j} + b\n",
    "$$\n",
    "\n",
    "Machine learning algorithms will generally perform best when their capacity is appropriate for the true complexity of the task they need to perform and the amount of training data they are provided with.\n",
    "\n",
    "![Underfitting and Overfitting](./Pictures/Capacity.png)\n",
    "\n",
    "The model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. This is called the representational capacity of the model.\n",
    "\n",
    "In practice, the learning algorithm does not actually ﬁnd the best function, but merely one that signiﬁcantly reduces the training error, this mean that the learning algorithm’s eﬀective capacity may be less than the representational capacity of the model family.\n",
    "\n",
    "The **Vapnik-Chervonenkis dimension** measures the capacity of a binary classiﬁer. The VC dimension is deﬁned as being the largest possible value of $m$ for which there exists a training set of $m$ diﬀerent $x$ points that the classiﬁer can label arbitrarily.\n",
    "\n",
    "![VC dimension](./Pictures/VC-Dimension.png)\n",
    "\n",
    "The statistical learning theory show that the discrepancy between training error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases\n",
    "\n",
    "Typically, training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value).\n",
    "\n",
    "![Relation Capacity Training Error](./Pictures/relation-capacity-generalization-error.png)\n",
    "\n",
    "At the left end of the graph, training error and generalization error are both high. This is the **underﬁtting** regime. As we increase capacity, training error decreases, but the gap between training and generalization error increases, and we enter the **overﬁtting** regime, where capacity is too large, above the **optimal capacity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric models learn a function describedby a parameter vector whose size is ﬁnite and ﬁxed before any data is observed. Nonparametric models have no such limitation.\n",
    "\n",
    "One example of such an algorithm is **nearest neighbor regression**. the nearest neighbor regression model simply stores the X and y from the training set. When asked to classify a test point $x$, the model looks up the nearest entry in the training set and returns the associated regression target. $y=y_i$ where $i=\\operatorname*{arg\\,min} ||X_{i} - x||^2_2$. (which might be greater than zero, if two identical inputs are associated with diﬀerent outputs) on any regression dataset. The algorithm can also be generalized to distance metrics other than the $L_2$ norm.\n",
    "\n",
    "we can also create a non parametric learning algorithm by wrapping a parametric learning algorithm inside another algorithm that increases the number of parameters as needed. For example, we could imagine an outer loop of learning that changes the degree of the polynomial learned by linear regression on top of a polynomial expansion of the input.\n",
    "\n",
    "The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from $x$ to $y$ may be inherently stochastic, or $y$ may be a deterministic function that involves other variables besides those included in $x$. The error incurred by an oracle making predictions from the true distribution $p(x, y)$ is called the **Bayes error**.\n",
    "\n",
    "Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples increases. For nonparametric models, more data yield better generalization until the best possible error is achieved. Any ﬁxed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. Note that it is possible for the model to have optimal capacity and yet still have a large gap between training and generalization errors. In this situation, we may be able to reduce this gap by gathering more training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning promises to ﬁnd rules that are probably correct about most members ofthe set they concern. The no free lunch theorem for machine learning states that no machine learning algorithm is universally any better than anyother. But if we make assumptions about the kinds of probabilitydistributions we encounter in real-world applications, then we can design learningalgorithms that perform well on these distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anBkSRMGqDS0"
   },
   "source": [
    "# Reference\n",
    "\n",
    "* [Deep Learning](http://www.deeplearningbook.org/)\n",
    "* [Math Basic](https://www.cis.upenn.edu/~jean/math-basics.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNuPpV6-lxx_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3RnR-hyyXN4i",
    "Ys6T3wE0-E8h",
    "9S_1fwFaJJnq"
   ],
   "name": "note.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
